{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fee05e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_692984/996909386.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"/media/yuganlab/blackstone/clip_classifier_epoch2.pt\", map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip_model.positional_embedding\n",
      "clip_model.text_projection\n",
      "clip_model.logit_scale\n",
      "clip_model.visual.class_embedding\n",
      "clip_model.visual.positional_embedding\n",
      "clip_model.visual.proj\n",
      "clip_model.visual.conv1.weight\n",
      "clip_model.visual.ln_pre.weight\n",
      "clip_model.visual.ln_pre.bias\n",
      "clip_model.visual.transformer.resblocks.0.ln_1.weight\n",
      "clip_model.visual.transformer.resblocks.0.ln_1.bias\n",
      "clip_model.visual.transformer.resblocks.0.attn.in_proj_weight\n",
      "clip_model.visual.transformer.resblocks.0.attn.in_proj_bias\n",
      "clip_model.visual.transformer.resblocks.0.attn.out_proj.weight\n",
      "clip_model.visual.transformer.resblocks.0.attn.out_proj.bias\n",
      "clip_model.visual.transformer.resblocks.0.ln_2.weight\n",
      "clip_model.visual.transformer.resblocks.0.ln_2.bias\n",
      "clip_model.visual.transformer.resblocks.0.mlp.c_fc.weight\n",
      "clip_model.visual.transformer.resblocks.0.mlp.c_fc.bias\n",
      "clip_model.visual.transformer.resblocks.0.mlp.c_proj.weight\n",
      "clip_model.visual.transformer.resblocks.0.mlp.c_proj.bias\n",
      "clip_model.visual.transformer.resblocks.1.ln_1.weight\n",
      "clip_model.visual.transformer.resblocks.1.ln_1.bias\n",
      "clip_model.visual.transformer.resblocks.1.attn.in_proj_weight\n",
      "clip_model.visual.transformer.resblocks.1.attn.in_proj_bias\n",
      "clip_model.visual.transformer.resblocks.1.attn.out_proj.weight\n",
      "clip_model.visual.transformer.resblocks.1.attn.out_proj.bias\n",
      "clip_model.visual.transformer.resblocks.1.ln_2.weight\n",
      "clip_model.visual.transformer.resblocks.1.ln_2.bias\n",
      "clip_model.visual.transformer.resblocks.1.mlp.c_fc.weight\n",
      "clip_model.visual.transformer.resblocks.1.mlp.c_fc.bias\n",
      "clip_model.visual.transformer.resblocks.1.mlp.c_proj.weight\n",
      "clip_model.visual.transformer.resblocks.1.mlp.c_proj.bias\n",
      "clip_model.visual.transformer.resblocks.2.ln_1.weight\n",
      "clip_model.visual.transformer.resblocks.2.ln_1.bias\n",
      "clip_model.visual.transformer.resblocks.2.attn.in_proj_weight\n",
      "clip_model.visual.transformer.resblocks.2.attn.in_proj_bias\n",
      "clip_model.visual.transformer.resblocks.2.attn.out_proj.weight\n",
      "clip_model.visual.transformer.resblocks.2.attn.out_proj.bias\n",
      "clip_model.visual.transformer.resblocks.2.ln_2.weight\n",
      "clip_model.visual.transformer.resblocks.2.ln_2.bias\n",
      "clip_model.visual.transformer.resblocks.2.mlp.c_fc.weight\n",
      "clip_model.visual.transformer.resblocks.2.mlp.c_fc.bias\n",
      "clip_model.visual.transformer.resblocks.2.mlp.c_proj.weight\n",
      "clip_model.visual.transformer.resblocks.2.mlp.c_proj.bias\n",
      "clip_model.visual.transformer.resblocks.3.ln_1.weight\n",
      "clip_model.visual.transformer.resblocks.3.ln_1.bias\n",
      "clip_model.visual.transformer.resblocks.3.attn.in_proj_weight\n",
      "clip_model.visual.transformer.resblocks.3.attn.in_proj_bias\n",
      "clip_model.visual.transformer.resblocks.3.attn.out_proj.weight\n",
      "clip_model.visual.transformer.resblocks.3.attn.out_proj.bias\n",
      "clip_model.visual.transformer.resblocks.3.ln_2.weight\n",
      "clip_model.visual.transformer.resblocks.3.ln_2.bias\n",
      "clip_model.visual.transformer.resblocks.3.mlp.c_fc.weight\n",
      "clip_model.visual.transformer.resblocks.3.mlp.c_fc.bias\n",
      "clip_model.visual.transformer.resblocks.3.mlp.c_proj.weight\n",
      "clip_model.visual.transformer.resblocks.3.mlp.c_proj.bias\n",
      "clip_model.visual.transformer.resblocks.4.ln_1.weight\n",
      "clip_model.visual.transformer.resblocks.4.ln_1.bias\n",
      "clip_model.visual.transformer.resblocks.4.attn.in_proj_weight\n",
      "clip_model.visual.transformer.resblocks.4.attn.in_proj_bias\n",
      "clip_model.visual.transformer.resblocks.4.attn.out_proj.weight\n",
      "clip_model.visual.transformer.resblocks.4.attn.out_proj.bias\n",
      "clip_model.visual.transformer.resblocks.4.ln_2.weight\n",
      "clip_model.visual.transformer.resblocks.4.ln_2.bias\n",
      "clip_model.visual.transformer.resblocks.4.mlp.c_fc.weight\n",
      "clip_model.visual.transformer.resblocks.4.mlp.c_fc.bias\n",
      "clip_model.visual.transformer.resblocks.4.mlp.c_proj.weight\n",
      "clip_model.visual.transformer.resblocks.4.mlp.c_proj.bias\n",
      "clip_model.visual.transformer.resblocks.5.ln_1.weight\n",
      "clip_model.visual.transformer.resblocks.5.ln_1.bias\n",
      "clip_model.visual.transformer.resblocks.5.attn.in_proj_weight\n",
      "clip_model.visual.transformer.resblocks.5.attn.in_proj_bias\n",
      "clip_model.visual.transformer.resblocks.5.attn.out_proj.weight\n",
      "clip_model.visual.transformer.resblocks.5.attn.out_proj.bias\n",
      "clip_model.visual.transformer.resblocks.5.ln_2.weight\n",
      "clip_model.visual.transformer.resblocks.5.ln_2.bias\n",
      "clip_model.visual.transformer.resblocks.5.mlp.c_fc.weight\n",
      "clip_model.visual.transformer.resblocks.5.mlp.c_fc.bias\n",
      "clip_model.visual.transformer.resblocks.5.mlp.c_proj.weight\n",
      "clip_model.visual.transformer.resblocks.5.mlp.c_proj.bias\n",
      "clip_model.visual.transformer.resblocks.6.ln_1.weight\n",
      "clip_model.visual.transformer.resblocks.6.ln_1.bias\n",
      "clip_model.visual.transformer.resblocks.6.attn.in_proj_weight\n",
      "clip_model.visual.transformer.resblocks.6.attn.in_proj_bias\n",
      "clip_model.visual.transformer.resblocks.6.attn.out_proj.weight\n",
      "clip_model.visual.transformer.resblocks.6.attn.out_proj.bias\n",
      "clip_model.visual.transformer.resblocks.6.ln_2.weight\n",
      "clip_model.visual.transformer.resblocks.6.ln_2.bias\n",
      "clip_model.visual.transformer.resblocks.6.mlp.c_fc.weight\n",
      "clip_model.visual.transformer.resblocks.6.mlp.c_fc.bias\n",
      "clip_model.visual.transformer.resblocks.6.mlp.c_proj.weight\n",
      "clip_model.visual.transformer.resblocks.6.mlp.c_proj.bias\n",
      "clip_model.visual.transformer.resblocks.7.ln_1.weight\n",
      "clip_model.visual.transformer.resblocks.7.ln_1.bias\n",
      "clip_model.visual.transformer.resblocks.7.attn.in_proj_weight\n",
      "clip_model.visual.transformer.resblocks.7.attn.in_proj_bias\n",
      "clip_model.visual.transformer.resblocks.7.attn.out_proj.weight\n",
      "clip_model.visual.transformer.resblocks.7.attn.out_proj.bias\n",
      "clip_model.visual.transformer.resblocks.7.ln_2.weight\n",
      "clip_model.visual.transformer.resblocks.7.ln_2.bias\n",
      "clip_model.visual.transformer.resblocks.7.mlp.c_fc.weight\n",
      "clip_model.visual.transformer.resblocks.7.mlp.c_fc.bias\n",
      "clip_model.visual.transformer.resblocks.7.mlp.c_proj.weight\n",
      "clip_model.visual.transformer.resblocks.7.mlp.c_proj.bias\n",
      "clip_model.visual.transformer.resblocks.8.ln_1.weight\n",
      "clip_model.visual.transformer.resblocks.8.ln_1.bias\n",
      "clip_model.visual.transformer.resblocks.8.attn.in_proj_weight\n",
      "clip_model.visual.transformer.resblocks.8.attn.in_proj_bias\n",
      "clip_model.visual.transformer.resblocks.8.attn.out_proj.weight\n",
      "clip_model.visual.transformer.resblocks.8.attn.out_proj.bias\n",
      "clip_model.visual.transformer.resblocks.8.ln_2.weight\n",
      "clip_model.visual.transformer.resblocks.8.ln_2.bias\n",
      "clip_model.visual.transformer.resblocks.8.mlp.c_fc.weight\n",
      "clip_model.visual.transformer.resblocks.8.mlp.c_fc.bias\n",
      "clip_model.visual.transformer.resblocks.8.mlp.c_proj.weight\n",
      "clip_model.visual.transformer.resblocks.8.mlp.c_proj.bias\n",
      "clip_model.visual.transformer.resblocks.9.ln_1.weight\n",
      "clip_model.visual.transformer.resblocks.9.ln_1.bias\n",
      "clip_model.visual.transformer.resblocks.9.attn.in_proj_weight\n",
      "clip_model.visual.transformer.resblocks.9.attn.in_proj_bias\n",
      "clip_model.visual.transformer.resblocks.9.attn.out_proj.weight\n",
      "clip_model.visual.transformer.resblocks.9.attn.out_proj.bias\n",
      "clip_model.visual.transformer.resblocks.9.ln_2.weight\n",
      "clip_model.visual.transformer.resblocks.9.ln_2.bias\n",
      "clip_model.visual.transformer.resblocks.9.mlp.c_fc.weight\n",
      "clip_model.visual.transformer.resblocks.9.mlp.c_fc.bias\n",
      "clip_model.visual.transformer.resblocks.9.mlp.c_proj.weight\n",
      "clip_model.visual.transformer.resblocks.9.mlp.c_proj.bias\n",
      "clip_model.visual.transformer.resblocks.10.ln_1.weight\n",
      "clip_model.visual.transformer.resblocks.10.ln_1.bias\n",
      "clip_model.visual.transformer.resblocks.10.attn.in_proj_weight\n",
      "clip_model.visual.transformer.resblocks.10.attn.in_proj_bias\n",
      "clip_model.visual.transformer.resblocks.10.attn.out_proj.weight\n",
      "clip_model.visual.transformer.resblocks.10.attn.out_proj.bias\n",
      "clip_model.visual.transformer.resblocks.10.ln_2.weight\n",
      "clip_model.visual.transformer.resblocks.10.ln_2.bias\n",
      "clip_model.visual.transformer.resblocks.10.mlp.c_fc.weight\n",
      "clip_model.visual.transformer.resblocks.10.mlp.c_fc.bias\n",
      "clip_model.visual.transformer.resblocks.10.mlp.c_proj.weight\n",
      "clip_model.visual.transformer.resblocks.10.mlp.c_proj.bias\n",
      "clip_model.visual.transformer.resblocks.11.ln_1.weight\n",
      "clip_model.visual.transformer.resblocks.11.ln_1.bias\n",
      "clip_model.visual.transformer.resblocks.11.attn.in_proj_weight\n",
      "clip_model.visual.transformer.resblocks.11.attn.in_proj_bias\n",
      "clip_model.visual.transformer.resblocks.11.attn.out_proj.weight\n",
      "clip_model.visual.transformer.resblocks.11.attn.out_proj.bias\n",
      "clip_model.visual.transformer.resblocks.11.ln_2.weight\n",
      "clip_model.visual.transformer.resblocks.11.ln_2.bias\n",
      "clip_model.visual.transformer.resblocks.11.mlp.c_fc.weight\n",
      "clip_model.visual.transformer.resblocks.11.mlp.c_fc.bias\n",
      "clip_model.visual.transformer.resblocks.11.mlp.c_proj.weight\n",
      "clip_model.visual.transformer.resblocks.11.mlp.c_proj.bias\n",
      "clip_model.visual.transformer.resblocks.12.ln_1.weight\n",
      "clip_model.visual.transformer.resblocks.12.ln_1.bias\n",
      "clip_model.visual.transformer.resblocks.12.attn.in_proj_weight\n",
      "clip_model.visual.transformer.resblocks.12.attn.in_proj_bias\n",
      "clip_model.visual.transformer.resblocks.12.attn.out_proj.weight\n",
      "clip_model.visual.transformer.resblocks.12.attn.out_proj.bias\n",
      "clip_model.visual.transformer.resblocks.12.ln_2.weight\n",
      "clip_model.visual.transformer.resblocks.12.ln_2.bias\n",
      "clip_model.visual.transformer.resblocks.12.mlp.c_fc.weight\n",
      "clip_model.visual.transformer.resblocks.12.mlp.c_fc.bias\n",
      "clip_model.visual.transformer.resblocks.12.mlp.c_proj.weight\n",
      "clip_model.visual.transformer.resblocks.12.mlp.c_proj.bias\n",
      "clip_model.visual.transformer.resblocks.13.ln_1.weight\n",
      "clip_model.visual.transformer.resblocks.13.ln_1.bias\n",
      "clip_model.visual.transformer.resblocks.13.attn.in_proj_weight\n",
      "clip_model.visual.transformer.resblocks.13.attn.in_proj_bias\n",
      "clip_model.visual.transformer.resblocks.13.attn.out_proj.weight\n",
      "clip_model.visual.transformer.resblocks.13.attn.out_proj.bias\n",
      "clip_model.visual.transformer.resblocks.13.ln_2.weight\n",
      "clip_model.visual.transformer.resblocks.13.ln_2.bias\n",
      "clip_model.visual.transformer.resblocks.13.mlp.c_fc.weight\n",
      "clip_model.visual.transformer.resblocks.13.mlp.c_fc.bias\n",
      "clip_model.visual.transformer.resblocks.13.mlp.c_proj.weight\n",
      "clip_model.visual.transformer.resblocks.13.mlp.c_proj.bias\n",
      "clip_model.visual.transformer.resblocks.14.ln_1.weight\n",
      "clip_model.visual.transformer.resblocks.14.ln_1.bias\n",
      "clip_model.visual.transformer.resblocks.14.attn.in_proj_weight\n",
      "clip_model.visual.transformer.resblocks.14.attn.in_proj_bias\n",
      "clip_model.visual.transformer.resblocks.14.attn.out_proj.weight\n",
      "clip_model.visual.transformer.resblocks.14.attn.out_proj.bias\n",
      "clip_model.visual.transformer.resblocks.14.ln_2.weight\n",
      "clip_model.visual.transformer.resblocks.14.ln_2.bias\n",
      "clip_model.visual.transformer.resblocks.14.mlp.c_fc.weight\n",
      "clip_model.visual.transformer.resblocks.14.mlp.c_fc.bias\n",
      "clip_model.visual.transformer.resblocks.14.mlp.c_proj.weight\n",
      "clip_model.visual.transformer.resblocks.14.mlp.c_proj.bias\n",
      "clip_model.visual.transformer.resblocks.15.ln_1.weight\n",
      "clip_model.visual.transformer.resblocks.15.ln_1.bias\n",
      "clip_model.visual.transformer.resblocks.15.attn.in_proj_weight\n",
      "clip_model.visual.transformer.resblocks.15.attn.in_proj_bias\n",
      "clip_model.visual.transformer.resblocks.15.attn.out_proj.weight\n",
      "clip_model.visual.transformer.resblocks.15.attn.out_proj.bias\n",
      "clip_model.visual.transformer.resblocks.15.ln_2.weight\n",
      "clip_model.visual.transformer.resblocks.15.ln_2.bias\n",
      "clip_model.visual.transformer.resblocks.15.mlp.c_fc.weight\n",
      "clip_model.visual.transformer.resblocks.15.mlp.c_fc.bias\n",
      "clip_model.visual.transformer.resblocks.15.mlp.c_proj.weight\n",
      "clip_model.visual.transformer.resblocks.15.mlp.c_proj.bias\n",
      "clip_model.visual.transformer.resblocks.16.ln_1.weight\n",
      "clip_model.visual.transformer.resblocks.16.ln_1.bias\n",
      "clip_model.visual.transformer.resblocks.16.attn.in_proj_weight\n",
      "clip_model.visual.transformer.resblocks.16.attn.in_proj_bias\n",
      "clip_model.visual.transformer.resblocks.16.attn.out_proj.weight\n",
      "clip_model.visual.transformer.resblocks.16.attn.out_proj.bias\n",
      "clip_model.visual.transformer.resblocks.16.ln_2.weight\n",
      "clip_model.visual.transformer.resblocks.16.ln_2.bias\n",
      "clip_model.visual.transformer.resblocks.16.mlp.c_fc.weight\n",
      "clip_model.visual.transformer.resblocks.16.mlp.c_fc.bias\n",
      "clip_model.visual.transformer.resblocks.16.mlp.c_proj.weight\n",
      "clip_model.visual.transformer.resblocks.16.mlp.c_proj.bias\n",
      "clip_model.visual.transformer.resblocks.17.ln_1.weight\n",
      "clip_model.visual.transformer.resblocks.17.ln_1.bias\n",
      "clip_model.visual.transformer.resblocks.17.attn.in_proj_weight\n",
      "clip_model.visual.transformer.resblocks.17.attn.in_proj_bias\n",
      "clip_model.visual.transformer.resblocks.17.attn.out_proj.weight\n",
      "clip_model.visual.transformer.resblocks.17.attn.out_proj.bias\n",
      "clip_model.visual.transformer.resblocks.17.ln_2.weight\n",
      "clip_model.visual.transformer.resblocks.17.ln_2.bias\n",
      "clip_model.visual.transformer.resblocks.17.mlp.c_fc.weight\n",
      "clip_model.visual.transformer.resblocks.17.mlp.c_fc.bias\n",
      "clip_model.visual.transformer.resblocks.17.mlp.c_proj.weight\n",
      "clip_model.visual.transformer.resblocks.17.mlp.c_proj.bias\n",
      "clip_model.visual.transformer.resblocks.18.ln_1.weight\n",
      "clip_model.visual.transformer.resblocks.18.ln_1.bias\n",
      "clip_model.visual.transformer.resblocks.18.attn.in_proj_weight\n",
      "clip_model.visual.transformer.resblocks.18.attn.in_proj_bias\n",
      "clip_model.visual.transformer.resblocks.18.attn.out_proj.weight\n",
      "clip_model.visual.transformer.resblocks.18.attn.out_proj.bias\n",
      "clip_model.visual.transformer.resblocks.18.ln_2.weight\n",
      "clip_model.visual.transformer.resblocks.18.ln_2.bias\n",
      "clip_model.visual.transformer.resblocks.18.mlp.c_fc.weight\n",
      "clip_model.visual.transformer.resblocks.18.mlp.c_fc.bias\n",
      "clip_model.visual.transformer.resblocks.18.mlp.c_proj.weight\n",
      "clip_model.visual.transformer.resblocks.18.mlp.c_proj.bias\n",
      "clip_model.visual.transformer.resblocks.19.ln_1.weight\n",
      "clip_model.visual.transformer.resblocks.19.ln_1.bias\n",
      "clip_model.visual.transformer.resblocks.19.attn.in_proj_weight\n",
      "clip_model.visual.transformer.resblocks.19.attn.in_proj_bias\n",
      "clip_model.visual.transformer.resblocks.19.attn.out_proj.weight\n",
      "clip_model.visual.transformer.resblocks.19.attn.out_proj.bias\n",
      "clip_model.visual.transformer.resblocks.19.ln_2.weight\n",
      "clip_model.visual.transformer.resblocks.19.ln_2.bias\n",
      "clip_model.visual.transformer.resblocks.19.mlp.c_fc.weight\n",
      "clip_model.visual.transformer.resblocks.19.mlp.c_fc.bias\n",
      "clip_model.visual.transformer.resblocks.19.mlp.c_proj.weight\n",
      "clip_model.visual.transformer.resblocks.19.mlp.c_proj.bias\n",
      "clip_model.visual.transformer.resblocks.20.ln_1.weight\n",
      "clip_model.visual.transformer.resblocks.20.ln_1.bias\n",
      "clip_model.visual.transformer.resblocks.20.attn.in_proj_weight\n",
      "clip_model.visual.transformer.resblocks.20.attn.in_proj_bias\n",
      "clip_model.visual.transformer.resblocks.20.attn.out_proj.weight\n",
      "clip_model.visual.transformer.resblocks.20.attn.out_proj.bias\n",
      "clip_model.visual.transformer.resblocks.20.ln_2.weight\n",
      "clip_model.visual.transformer.resblocks.20.ln_2.bias\n",
      "clip_model.visual.transformer.resblocks.20.mlp.c_fc.weight\n",
      "clip_model.visual.transformer.resblocks.20.mlp.c_fc.bias\n",
      "clip_model.visual.transformer.resblocks.20.mlp.c_proj.weight\n",
      "clip_model.visual.transformer.resblocks.20.mlp.c_proj.bias\n",
      "clip_model.visual.transformer.resblocks.21.ln_1.weight\n",
      "clip_model.visual.transformer.resblocks.21.ln_1.bias\n",
      "clip_model.visual.transformer.resblocks.21.attn.in_proj_weight\n",
      "clip_model.visual.transformer.resblocks.21.attn.in_proj_bias\n",
      "clip_model.visual.transformer.resblocks.21.attn.out_proj.weight\n",
      "clip_model.visual.transformer.resblocks.21.attn.out_proj.bias\n",
      "clip_model.visual.transformer.resblocks.21.ln_2.weight\n",
      "clip_model.visual.transformer.resblocks.21.ln_2.bias\n",
      "clip_model.visual.transformer.resblocks.21.mlp.c_fc.weight\n",
      "clip_model.visual.transformer.resblocks.21.mlp.c_fc.bias\n",
      "clip_model.visual.transformer.resblocks.21.mlp.c_proj.weight\n",
      "clip_model.visual.transformer.resblocks.21.mlp.c_proj.bias\n",
      "clip_model.visual.transformer.resblocks.22.ln_1.weight\n",
      "clip_model.visual.transformer.resblocks.22.ln_1.bias\n",
      "clip_model.visual.transformer.resblocks.22.attn.in_proj_weight\n",
      "clip_model.visual.transformer.resblocks.22.attn.in_proj_bias\n",
      "clip_model.visual.transformer.resblocks.22.attn.out_proj.weight\n",
      "clip_model.visual.transformer.resblocks.22.attn.out_proj.bias\n",
      "clip_model.visual.transformer.resblocks.22.ln_2.weight\n",
      "clip_model.visual.transformer.resblocks.22.ln_2.bias\n",
      "clip_model.visual.transformer.resblocks.22.mlp.c_fc.weight\n",
      "clip_model.visual.transformer.resblocks.22.mlp.c_fc.bias\n",
      "clip_model.visual.transformer.resblocks.22.mlp.c_proj.weight\n",
      "clip_model.visual.transformer.resblocks.22.mlp.c_proj.bias\n",
      "clip_model.visual.transformer.resblocks.23.ln_1.weight\n",
      "clip_model.visual.transformer.resblocks.23.ln_1.bias\n",
      "clip_model.visual.transformer.resblocks.23.attn.in_proj_weight\n",
      "clip_model.visual.transformer.resblocks.23.attn.in_proj_bias\n",
      "clip_model.visual.transformer.resblocks.23.attn.out_proj.weight\n",
      "clip_model.visual.transformer.resblocks.23.attn.out_proj.bias\n",
      "clip_model.visual.transformer.resblocks.23.ln_2.weight\n",
      "clip_model.visual.transformer.resblocks.23.ln_2.bias\n",
      "clip_model.visual.transformer.resblocks.23.mlp.c_fc.weight\n",
      "clip_model.visual.transformer.resblocks.23.mlp.c_fc.bias\n",
      "clip_model.visual.transformer.resblocks.23.mlp.c_proj.weight\n",
      "clip_model.visual.transformer.resblocks.23.mlp.c_proj.bias\n",
      "clip_model.visual.transformer.resblocks.24.ln_1.weight\n",
      "clip_model.visual.transformer.resblocks.24.ln_1.bias\n",
      "clip_model.visual.transformer.resblocks.24.attn.in_proj_weight\n",
      "clip_model.visual.transformer.resblocks.24.attn.in_proj_bias\n",
      "clip_model.visual.transformer.resblocks.24.attn.out_proj.weight\n",
      "clip_model.visual.transformer.resblocks.24.attn.out_proj.bias\n",
      "clip_model.visual.transformer.resblocks.24.ln_2.weight\n",
      "clip_model.visual.transformer.resblocks.24.ln_2.bias\n",
      "clip_model.visual.transformer.resblocks.24.mlp.c_fc.weight\n",
      "clip_model.visual.transformer.resblocks.24.mlp.c_fc.bias\n",
      "clip_model.visual.transformer.resblocks.24.mlp.c_proj.weight\n",
      "clip_model.visual.transformer.resblocks.24.mlp.c_proj.bias\n",
      "clip_model.visual.transformer.resblocks.25.ln_1.weight\n",
      "clip_model.visual.transformer.resblocks.25.ln_1.bias\n",
      "clip_model.visual.transformer.resblocks.25.attn.in_proj_weight\n",
      "clip_model.visual.transformer.resblocks.25.attn.in_proj_bias\n",
      "clip_model.visual.transformer.resblocks.25.attn.out_proj.weight\n",
      "clip_model.visual.transformer.resblocks.25.attn.out_proj.bias\n",
      "clip_model.visual.transformer.resblocks.25.ln_2.weight\n",
      "clip_model.visual.transformer.resblocks.25.ln_2.bias\n",
      "clip_model.visual.transformer.resblocks.25.mlp.c_fc.weight\n",
      "clip_model.visual.transformer.resblocks.25.mlp.c_fc.bias\n",
      "clip_model.visual.transformer.resblocks.25.mlp.c_proj.weight\n",
      "clip_model.visual.transformer.resblocks.25.mlp.c_proj.bias\n",
      "clip_model.visual.transformer.resblocks.26.ln_1.weight\n",
      "clip_model.visual.transformer.resblocks.26.ln_1.bias\n",
      "clip_model.visual.transformer.resblocks.26.attn.in_proj_weight\n",
      "clip_model.visual.transformer.resblocks.26.attn.in_proj_bias\n",
      "clip_model.visual.transformer.resblocks.26.attn.out_proj.weight\n",
      "clip_model.visual.transformer.resblocks.26.attn.out_proj.bias\n",
      "clip_model.visual.transformer.resblocks.26.ln_2.weight\n",
      "clip_model.visual.transformer.resblocks.26.ln_2.bias\n",
      "clip_model.visual.transformer.resblocks.26.mlp.c_fc.weight\n",
      "clip_model.visual.transformer.resblocks.26.mlp.c_fc.bias\n",
      "clip_model.visual.transformer.resblocks.26.mlp.c_proj.weight\n",
      "clip_model.visual.transformer.resblocks.26.mlp.c_proj.bias\n",
      "clip_model.visual.transformer.resblocks.27.ln_1.weight\n",
      "clip_model.visual.transformer.resblocks.27.ln_1.bias\n",
      "clip_model.visual.transformer.resblocks.27.attn.in_proj_weight\n",
      "clip_model.visual.transformer.resblocks.27.attn.in_proj_bias\n",
      "clip_model.visual.transformer.resblocks.27.attn.out_proj.weight\n",
      "clip_model.visual.transformer.resblocks.27.attn.out_proj.bias\n",
      "clip_model.visual.transformer.resblocks.27.ln_2.weight\n",
      "clip_model.visual.transformer.resblocks.27.ln_2.bias\n",
      "clip_model.visual.transformer.resblocks.27.mlp.c_fc.weight\n",
      "clip_model.visual.transformer.resblocks.27.mlp.c_fc.bias\n",
      "clip_model.visual.transformer.resblocks.27.mlp.c_proj.weight\n",
      "clip_model.visual.transformer.resblocks.27.mlp.c_proj.bias\n",
      "clip_model.visual.transformer.resblocks.28.ln_1.weight\n",
      "clip_model.visual.transformer.resblocks.28.ln_1.bias\n",
      "clip_model.visual.transformer.resblocks.28.attn.in_proj_weight\n",
      "clip_model.visual.transformer.resblocks.28.attn.in_proj_bias\n",
      "clip_model.visual.transformer.resblocks.28.attn.out_proj.weight\n",
      "clip_model.visual.transformer.resblocks.28.attn.out_proj.bias\n",
      "clip_model.visual.transformer.resblocks.28.ln_2.weight\n",
      "clip_model.visual.transformer.resblocks.28.ln_2.bias\n",
      "clip_model.visual.transformer.resblocks.28.mlp.c_fc.weight\n",
      "clip_model.visual.transformer.resblocks.28.mlp.c_fc.bias\n",
      "clip_model.visual.transformer.resblocks.28.mlp.c_proj.weight\n",
      "clip_model.visual.transformer.resblocks.28.mlp.c_proj.bias\n",
      "clip_model.visual.transformer.resblocks.29.ln_1.weight\n",
      "clip_model.visual.transformer.resblocks.29.ln_1.bias\n",
      "clip_model.visual.transformer.resblocks.29.attn.in_proj_weight\n",
      "clip_model.visual.transformer.resblocks.29.attn.in_proj_bias\n",
      "clip_model.visual.transformer.resblocks.29.attn.out_proj.weight\n",
      "clip_model.visual.transformer.resblocks.29.attn.out_proj.bias\n",
      "clip_model.visual.transformer.resblocks.29.ln_2.weight\n",
      "clip_model.visual.transformer.resblocks.29.ln_2.bias\n",
      "clip_model.visual.transformer.resblocks.29.mlp.c_fc.weight\n",
      "clip_model.visual.transformer.resblocks.29.mlp.c_fc.bias\n",
      "clip_model.visual.transformer.resblocks.29.mlp.c_proj.weight\n",
      "clip_model.visual.transformer.resblocks.29.mlp.c_proj.bias\n",
      "clip_model.visual.transformer.resblocks.30.ln_1.weight\n",
      "clip_model.visual.transformer.resblocks.30.ln_1.bias\n",
      "clip_model.visual.transformer.resblocks.30.attn.in_proj_weight\n",
      "clip_model.visual.transformer.resblocks.30.attn.in_proj_bias\n",
      "clip_model.visual.transformer.resblocks.30.attn.out_proj.weight\n",
      "clip_model.visual.transformer.resblocks.30.attn.out_proj.bias\n",
      "clip_model.visual.transformer.resblocks.30.ln_2.weight\n",
      "clip_model.visual.transformer.resblocks.30.ln_2.bias\n",
      "clip_model.visual.transformer.resblocks.30.mlp.c_fc.weight\n",
      "clip_model.visual.transformer.resblocks.30.mlp.c_fc.bias\n",
      "clip_model.visual.transformer.resblocks.30.mlp.c_proj.weight\n",
      "clip_model.visual.transformer.resblocks.30.mlp.c_proj.bias\n",
      "clip_model.visual.transformer.resblocks.31.ln_1.weight\n",
      "clip_model.visual.transformer.resblocks.31.ln_1.bias\n",
      "clip_model.visual.transformer.resblocks.31.attn.in_proj_weight\n",
      "clip_model.visual.transformer.resblocks.31.attn.in_proj_bias\n",
      "clip_model.visual.transformer.resblocks.31.attn.out_proj.weight\n",
      "clip_model.visual.transformer.resblocks.31.attn.out_proj.bias\n",
      "clip_model.visual.transformer.resblocks.31.ln_2.weight\n",
      "clip_model.visual.transformer.resblocks.31.ln_2.bias\n",
      "clip_model.visual.transformer.resblocks.31.mlp.c_fc.weight\n",
      "clip_model.visual.transformer.resblocks.31.mlp.c_fc.bias\n",
      "clip_model.visual.transformer.resblocks.31.mlp.c_proj.weight\n",
      "clip_model.visual.transformer.resblocks.31.mlp.c_proj.bias\n",
      "clip_model.visual.ln_post.weight\n",
      "clip_model.visual.ln_post.bias\n",
      "clip_model.transformer.resblocks.0.ln_1.weight\n",
      "clip_model.transformer.resblocks.0.ln_1.bias\n",
      "clip_model.transformer.resblocks.0.attn.in_proj_weight\n",
      "clip_model.transformer.resblocks.0.attn.in_proj_bias\n",
      "clip_model.transformer.resblocks.0.attn.out_proj.weight\n",
      "clip_model.transformer.resblocks.0.attn.out_proj.bias\n",
      "clip_model.transformer.resblocks.0.ln_2.weight\n",
      "clip_model.transformer.resblocks.0.ln_2.bias\n",
      "clip_model.transformer.resblocks.0.mlp.c_fc.weight\n",
      "clip_model.transformer.resblocks.0.mlp.c_fc.bias\n",
      "clip_model.transformer.resblocks.0.mlp.c_proj.weight\n",
      "clip_model.transformer.resblocks.0.mlp.c_proj.bias\n",
      "clip_model.transformer.resblocks.1.ln_1.weight\n",
      "clip_model.transformer.resblocks.1.ln_1.bias\n",
      "clip_model.transformer.resblocks.1.attn.in_proj_weight\n",
      "clip_model.transformer.resblocks.1.attn.in_proj_bias\n",
      "clip_model.transformer.resblocks.1.attn.out_proj.weight\n",
      "clip_model.transformer.resblocks.1.attn.out_proj.bias\n",
      "clip_model.transformer.resblocks.1.ln_2.weight\n",
      "clip_model.transformer.resblocks.1.ln_2.bias\n",
      "clip_model.transformer.resblocks.1.mlp.c_fc.weight\n",
      "clip_model.transformer.resblocks.1.mlp.c_fc.bias\n",
      "clip_model.transformer.resblocks.1.mlp.c_proj.weight\n",
      "clip_model.transformer.resblocks.1.mlp.c_proj.bias\n",
      "clip_model.transformer.resblocks.2.ln_1.weight\n",
      "clip_model.transformer.resblocks.2.ln_1.bias\n",
      "clip_model.transformer.resblocks.2.attn.in_proj_weight\n",
      "clip_model.transformer.resblocks.2.attn.in_proj_bias\n",
      "clip_model.transformer.resblocks.2.attn.out_proj.weight\n",
      "clip_model.transformer.resblocks.2.attn.out_proj.bias\n",
      "clip_model.transformer.resblocks.2.ln_2.weight\n",
      "clip_model.transformer.resblocks.2.ln_2.bias\n",
      "clip_model.transformer.resblocks.2.mlp.c_fc.weight\n",
      "clip_model.transformer.resblocks.2.mlp.c_fc.bias\n",
      "clip_model.transformer.resblocks.2.mlp.c_proj.weight\n",
      "clip_model.transformer.resblocks.2.mlp.c_proj.bias\n",
      "clip_model.transformer.resblocks.3.ln_1.weight\n",
      "clip_model.transformer.resblocks.3.ln_1.bias\n",
      "clip_model.transformer.resblocks.3.attn.in_proj_weight\n",
      "clip_model.transformer.resblocks.3.attn.in_proj_bias\n",
      "clip_model.transformer.resblocks.3.attn.out_proj.weight\n",
      "clip_model.transformer.resblocks.3.attn.out_proj.bias\n",
      "clip_model.transformer.resblocks.3.ln_2.weight\n",
      "clip_model.transformer.resblocks.3.ln_2.bias\n",
      "clip_model.transformer.resblocks.3.mlp.c_fc.weight\n",
      "clip_model.transformer.resblocks.3.mlp.c_fc.bias\n",
      "clip_model.transformer.resblocks.3.mlp.c_proj.weight\n",
      "clip_model.transformer.resblocks.3.mlp.c_proj.bias\n",
      "clip_model.transformer.resblocks.4.ln_1.weight\n",
      "clip_model.transformer.resblocks.4.ln_1.bias\n",
      "clip_model.transformer.resblocks.4.attn.in_proj_weight\n",
      "clip_model.transformer.resblocks.4.attn.in_proj_bias\n",
      "clip_model.transformer.resblocks.4.attn.out_proj.weight\n",
      "clip_model.transformer.resblocks.4.attn.out_proj.bias\n",
      "clip_model.transformer.resblocks.4.ln_2.weight\n",
      "clip_model.transformer.resblocks.4.ln_2.bias\n",
      "clip_model.transformer.resblocks.4.mlp.c_fc.weight\n",
      "clip_model.transformer.resblocks.4.mlp.c_fc.bias\n",
      "clip_model.transformer.resblocks.4.mlp.c_proj.weight\n",
      "clip_model.transformer.resblocks.4.mlp.c_proj.bias\n",
      "clip_model.transformer.resblocks.5.ln_1.weight\n",
      "clip_model.transformer.resblocks.5.ln_1.bias\n",
      "clip_model.transformer.resblocks.5.attn.in_proj_weight\n",
      "clip_model.transformer.resblocks.5.attn.in_proj_bias\n",
      "clip_model.transformer.resblocks.5.attn.out_proj.weight\n",
      "clip_model.transformer.resblocks.5.attn.out_proj.bias\n",
      "clip_model.transformer.resblocks.5.ln_2.weight\n",
      "clip_model.transformer.resblocks.5.ln_2.bias\n",
      "clip_model.transformer.resblocks.5.mlp.c_fc.weight\n",
      "clip_model.transformer.resblocks.5.mlp.c_fc.bias\n",
      "clip_model.transformer.resblocks.5.mlp.c_proj.weight\n",
      "clip_model.transformer.resblocks.5.mlp.c_proj.bias\n",
      "clip_model.transformer.resblocks.6.ln_1.weight\n",
      "clip_model.transformer.resblocks.6.ln_1.bias\n",
      "clip_model.transformer.resblocks.6.attn.in_proj_weight\n",
      "clip_model.transformer.resblocks.6.attn.in_proj_bias\n",
      "clip_model.transformer.resblocks.6.attn.out_proj.weight\n",
      "clip_model.transformer.resblocks.6.attn.out_proj.bias\n",
      "clip_model.transformer.resblocks.6.ln_2.weight\n",
      "clip_model.transformer.resblocks.6.ln_2.bias\n",
      "clip_model.transformer.resblocks.6.mlp.c_fc.weight\n",
      "clip_model.transformer.resblocks.6.mlp.c_fc.bias\n",
      "clip_model.transformer.resblocks.6.mlp.c_proj.weight\n",
      "clip_model.transformer.resblocks.6.mlp.c_proj.bias\n",
      "clip_model.transformer.resblocks.7.ln_1.weight\n",
      "clip_model.transformer.resblocks.7.ln_1.bias\n",
      "clip_model.transformer.resblocks.7.attn.in_proj_weight\n",
      "clip_model.transformer.resblocks.7.attn.in_proj_bias\n",
      "clip_model.transformer.resblocks.7.attn.out_proj.weight\n",
      "clip_model.transformer.resblocks.7.attn.out_proj.bias\n",
      "clip_model.transformer.resblocks.7.ln_2.weight\n",
      "clip_model.transformer.resblocks.7.ln_2.bias\n",
      "clip_model.transformer.resblocks.7.mlp.c_fc.weight\n",
      "clip_model.transformer.resblocks.7.mlp.c_fc.bias\n",
      "clip_model.transformer.resblocks.7.mlp.c_proj.weight\n",
      "clip_model.transformer.resblocks.7.mlp.c_proj.bias\n",
      "clip_model.transformer.resblocks.8.ln_1.weight\n",
      "clip_model.transformer.resblocks.8.ln_1.bias\n",
      "clip_model.transformer.resblocks.8.attn.in_proj_weight\n",
      "clip_model.transformer.resblocks.8.attn.in_proj_bias\n",
      "clip_model.transformer.resblocks.8.attn.out_proj.weight\n",
      "clip_model.transformer.resblocks.8.attn.out_proj.bias\n",
      "clip_model.transformer.resblocks.8.ln_2.weight\n",
      "clip_model.transformer.resblocks.8.ln_2.bias\n",
      "clip_model.transformer.resblocks.8.mlp.c_fc.weight\n",
      "clip_model.transformer.resblocks.8.mlp.c_fc.bias\n",
      "clip_model.transformer.resblocks.8.mlp.c_proj.weight\n",
      "clip_model.transformer.resblocks.8.mlp.c_proj.bias\n",
      "clip_model.transformer.resblocks.9.ln_1.weight\n",
      "clip_model.transformer.resblocks.9.ln_1.bias\n",
      "clip_model.transformer.resblocks.9.attn.in_proj_weight\n",
      "clip_model.transformer.resblocks.9.attn.in_proj_bias\n",
      "clip_model.transformer.resblocks.9.attn.out_proj.weight\n",
      "clip_model.transformer.resblocks.9.attn.out_proj.bias\n",
      "clip_model.transformer.resblocks.9.ln_2.weight\n",
      "clip_model.transformer.resblocks.9.ln_2.bias\n",
      "clip_model.transformer.resblocks.9.mlp.c_fc.weight\n",
      "clip_model.transformer.resblocks.9.mlp.c_fc.bias\n",
      "clip_model.transformer.resblocks.9.mlp.c_proj.weight\n",
      "clip_model.transformer.resblocks.9.mlp.c_proj.bias\n",
      "clip_model.transformer.resblocks.10.ln_1.weight\n",
      "clip_model.transformer.resblocks.10.ln_1.bias\n",
      "clip_model.transformer.resblocks.10.attn.in_proj_weight\n",
      "clip_model.transformer.resblocks.10.attn.in_proj_bias\n",
      "clip_model.transformer.resblocks.10.attn.out_proj.weight\n",
      "clip_model.transformer.resblocks.10.attn.out_proj.bias\n",
      "clip_model.transformer.resblocks.10.ln_2.weight\n",
      "clip_model.transformer.resblocks.10.ln_2.bias\n",
      "clip_model.transformer.resblocks.10.mlp.c_fc.weight\n",
      "clip_model.transformer.resblocks.10.mlp.c_fc.bias\n",
      "clip_model.transformer.resblocks.10.mlp.c_proj.weight\n",
      "clip_model.transformer.resblocks.10.mlp.c_proj.bias\n",
      "clip_model.transformer.resblocks.11.ln_1.weight\n",
      "clip_model.transformer.resblocks.11.ln_1.bias\n",
      "clip_model.transformer.resblocks.11.attn.in_proj_weight\n",
      "clip_model.transformer.resblocks.11.attn.in_proj_bias\n",
      "clip_model.transformer.resblocks.11.attn.out_proj.weight\n",
      "clip_model.transformer.resblocks.11.attn.out_proj.bias\n",
      "clip_model.transformer.resblocks.11.ln_2.weight\n",
      "clip_model.transformer.resblocks.11.ln_2.bias\n",
      "clip_model.transformer.resblocks.11.mlp.c_fc.weight\n",
      "clip_model.transformer.resblocks.11.mlp.c_fc.bias\n",
      "clip_model.transformer.resblocks.11.mlp.c_proj.weight\n",
      "clip_model.transformer.resblocks.11.mlp.c_proj.bias\n",
      "clip_model.transformer.resblocks.12.ln_1.weight\n",
      "clip_model.transformer.resblocks.12.ln_1.bias\n",
      "clip_model.transformer.resblocks.12.attn.in_proj_weight\n",
      "clip_model.transformer.resblocks.12.attn.in_proj_bias\n",
      "clip_model.transformer.resblocks.12.attn.out_proj.weight\n",
      "clip_model.transformer.resblocks.12.attn.out_proj.bias\n",
      "clip_model.transformer.resblocks.12.ln_2.weight\n",
      "clip_model.transformer.resblocks.12.ln_2.bias\n",
      "clip_model.transformer.resblocks.12.mlp.c_fc.weight\n",
      "clip_model.transformer.resblocks.12.mlp.c_fc.bias\n",
      "clip_model.transformer.resblocks.12.mlp.c_proj.weight\n",
      "clip_model.transformer.resblocks.12.mlp.c_proj.bias\n",
      "clip_model.transformer.resblocks.13.ln_1.weight\n",
      "clip_model.transformer.resblocks.13.ln_1.bias\n",
      "clip_model.transformer.resblocks.13.attn.in_proj_weight\n",
      "clip_model.transformer.resblocks.13.attn.in_proj_bias\n",
      "clip_model.transformer.resblocks.13.attn.out_proj.weight\n",
      "clip_model.transformer.resblocks.13.attn.out_proj.bias\n",
      "clip_model.transformer.resblocks.13.ln_2.weight\n",
      "clip_model.transformer.resblocks.13.ln_2.bias\n",
      "clip_model.transformer.resblocks.13.mlp.c_fc.weight\n",
      "clip_model.transformer.resblocks.13.mlp.c_fc.bias\n",
      "clip_model.transformer.resblocks.13.mlp.c_proj.weight\n",
      "clip_model.transformer.resblocks.13.mlp.c_proj.bias\n",
      "clip_model.transformer.resblocks.14.ln_1.weight\n",
      "clip_model.transformer.resblocks.14.ln_1.bias\n",
      "clip_model.transformer.resblocks.14.attn.in_proj_weight\n",
      "clip_model.transformer.resblocks.14.attn.in_proj_bias\n",
      "clip_model.transformer.resblocks.14.attn.out_proj.weight\n",
      "clip_model.transformer.resblocks.14.attn.out_proj.bias\n",
      "clip_model.transformer.resblocks.14.ln_2.weight\n",
      "clip_model.transformer.resblocks.14.ln_2.bias\n",
      "clip_model.transformer.resblocks.14.mlp.c_fc.weight\n",
      "clip_model.transformer.resblocks.14.mlp.c_fc.bias\n",
      "clip_model.transformer.resblocks.14.mlp.c_proj.weight\n",
      "clip_model.transformer.resblocks.14.mlp.c_proj.bias\n",
      "clip_model.transformer.resblocks.15.ln_1.weight\n",
      "clip_model.transformer.resblocks.15.ln_1.bias\n",
      "clip_model.transformer.resblocks.15.attn.in_proj_weight\n",
      "clip_model.transformer.resblocks.15.attn.in_proj_bias\n",
      "clip_model.transformer.resblocks.15.attn.out_proj.weight\n",
      "clip_model.transformer.resblocks.15.attn.out_proj.bias\n",
      "clip_model.transformer.resblocks.15.ln_2.weight\n",
      "clip_model.transformer.resblocks.15.ln_2.bias\n",
      "clip_model.transformer.resblocks.15.mlp.c_fc.weight\n",
      "clip_model.transformer.resblocks.15.mlp.c_fc.bias\n",
      "clip_model.transformer.resblocks.15.mlp.c_proj.weight\n",
      "clip_model.transformer.resblocks.15.mlp.c_proj.bias\n",
      "clip_model.transformer.resblocks.16.ln_1.weight\n",
      "clip_model.transformer.resblocks.16.ln_1.bias\n",
      "clip_model.transformer.resblocks.16.attn.in_proj_weight\n",
      "clip_model.transformer.resblocks.16.attn.in_proj_bias\n",
      "clip_model.transformer.resblocks.16.attn.out_proj.weight\n",
      "clip_model.transformer.resblocks.16.attn.out_proj.bias\n",
      "clip_model.transformer.resblocks.16.ln_2.weight\n",
      "clip_model.transformer.resblocks.16.ln_2.bias\n",
      "clip_model.transformer.resblocks.16.mlp.c_fc.weight\n",
      "clip_model.transformer.resblocks.16.mlp.c_fc.bias\n",
      "clip_model.transformer.resblocks.16.mlp.c_proj.weight\n",
      "clip_model.transformer.resblocks.16.mlp.c_proj.bias\n",
      "clip_model.transformer.resblocks.17.ln_1.weight\n",
      "clip_model.transformer.resblocks.17.ln_1.bias\n",
      "clip_model.transformer.resblocks.17.attn.in_proj_weight\n",
      "clip_model.transformer.resblocks.17.attn.in_proj_bias\n",
      "clip_model.transformer.resblocks.17.attn.out_proj.weight\n",
      "clip_model.transformer.resblocks.17.attn.out_proj.bias\n",
      "clip_model.transformer.resblocks.17.ln_2.weight\n",
      "clip_model.transformer.resblocks.17.ln_2.bias\n",
      "clip_model.transformer.resblocks.17.mlp.c_fc.weight\n",
      "clip_model.transformer.resblocks.17.mlp.c_fc.bias\n",
      "clip_model.transformer.resblocks.17.mlp.c_proj.weight\n",
      "clip_model.transformer.resblocks.17.mlp.c_proj.bias\n",
      "clip_model.transformer.resblocks.18.ln_1.weight\n",
      "clip_model.transformer.resblocks.18.ln_1.bias\n",
      "clip_model.transformer.resblocks.18.attn.in_proj_weight\n",
      "clip_model.transformer.resblocks.18.attn.in_proj_bias\n",
      "clip_model.transformer.resblocks.18.attn.out_proj.weight\n",
      "clip_model.transformer.resblocks.18.attn.out_proj.bias\n",
      "clip_model.transformer.resblocks.18.ln_2.weight\n",
      "clip_model.transformer.resblocks.18.ln_2.bias\n",
      "clip_model.transformer.resblocks.18.mlp.c_fc.weight\n",
      "clip_model.transformer.resblocks.18.mlp.c_fc.bias\n",
      "clip_model.transformer.resblocks.18.mlp.c_proj.weight\n",
      "clip_model.transformer.resblocks.18.mlp.c_proj.bias\n",
      "clip_model.transformer.resblocks.19.ln_1.weight\n",
      "clip_model.transformer.resblocks.19.ln_1.bias\n",
      "clip_model.transformer.resblocks.19.attn.in_proj_weight\n",
      "clip_model.transformer.resblocks.19.attn.in_proj_bias\n",
      "clip_model.transformer.resblocks.19.attn.out_proj.weight\n",
      "clip_model.transformer.resblocks.19.attn.out_proj.bias\n",
      "clip_model.transformer.resblocks.19.ln_2.weight\n",
      "clip_model.transformer.resblocks.19.ln_2.bias\n",
      "clip_model.transformer.resblocks.19.mlp.c_fc.weight\n",
      "clip_model.transformer.resblocks.19.mlp.c_fc.bias\n",
      "clip_model.transformer.resblocks.19.mlp.c_proj.weight\n",
      "clip_model.transformer.resblocks.19.mlp.c_proj.bias\n",
      "clip_model.transformer.resblocks.20.ln_1.weight\n",
      "clip_model.transformer.resblocks.20.ln_1.bias\n",
      "clip_model.transformer.resblocks.20.attn.in_proj_weight\n",
      "clip_model.transformer.resblocks.20.attn.in_proj_bias\n",
      "clip_model.transformer.resblocks.20.attn.out_proj.weight\n",
      "clip_model.transformer.resblocks.20.attn.out_proj.bias\n",
      "clip_model.transformer.resblocks.20.ln_2.weight\n",
      "clip_model.transformer.resblocks.20.ln_2.bias\n",
      "clip_model.transformer.resblocks.20.mlp.c_fc.weight\n",
      "clip_model.transformer.resblocks.20.mlp.c_fc.bias\n",
      "clip_model.transformer.resblocks.20.mlp.c_proj.weight\n",
      "clip_model.transformer.resblocks.20.mlp.c_proj.bias\n",
      "clip_model.transformer.resblocks.21.ln_1.weight\n",
      "clip_model.transformer.resblocks.21.ln_1.bias\n",
      "clip_model.transformer.resblocks.21.attn.in_proj_weight\n",
      "clip_model.transformer.resblocks.21.attn.in_proj_bias\n",
      "clip_model.transformer.resblocks.21.attn.out_proj.weight\n",
      "clip_model.transformer.resblocks.21.attn.out_proj.bias\n",
      "clip_model.transformer.resblocks.21.ln_2.weight\n",
      "clip_model.transformer.resblocks.21.ln_2.bias\n",
      "clip_model.transformer.resblocks.21.mlp.c_fc.weight\n",
      "clip_model.transformer.resblocks.21.mlp.c_fc.bias\n",
      "clip_model.transformer.resblocks.21.mlp.c_proj.weight\n",
      "clip_model.transformer.resblocks.21.mlp.c_proj.bias\n",
      "clip_model.transformer.resblocks.22.ln_1.weight\n",
      "clip_model.transformer.resblocks.22.ln_1.bias\n",
      "clip_model.transformer.resblocks.22.attn.in_proj_weight\n",
      "clip_model.transformer.resblocks.22.attn.in_proj_bias\n",
      "clip_model.transformer.resblocks.22.attn.out_proj.weight\n",
      "clip_model.transformer.resblocks.22.attn.out_proj.bias\n",
      "clip_model.transformer.resblocks.22.ln_2.weight\n",
      "clip_model.transformer.resblocks.22.ln_2.bias\n",
      "clip_model.transformer.resblocks.22.mlp.c_fc.weight\n",
      "clip_model.transformer.resblocks.22.mlp.c_fc.bias\n",
      "clip_model.transformer.resblocks.22.mlp.c_proj.weight\n",
      "clip_model.transformer.resblocks.22.mlp.c_proj.bias\n",
      "clip_model.transformer.resblocks.23.ln_1.weight\n",
      "clip_model.transformer.resblocks.23.ln_1.bias\n",
      "clip_model.transformer.resblocks.23.attn.in_proj_weight\n",
      "clip_model.transformer.resblocks.23.attn.in_proj_bias\n",
      "clip_model.transformer.resblocks.23.attn.out_proj.weight\n",
      "clip_model.transformer.resblocks.23.attn.out_proj.bias\n",
      "clip_model.transformer.resblocks.23.ln_2.weight\n",
      "clip_model.transformer.resblocks.23.ln_2.bias\n",
      "clip_model.transformer.resblocks.23.mlp.c_fc.weight\n",
      "clip_model.transformer.resblocks.23.mlp.c_fc.bias\n",
      "clip_model.transformer.resblocks.23.mlp.c_proj.weight\n",
      "clip_model.transformer.resblocks.23.mlp.c_proj.bias\n",
      "clip_model.token_embedding.weight\n",
      "clip_model.ln_final.weight\n",
      "clip_model.ln_final.bias\n",
      "classifier.0.weight\n",
      "classifier.0.bias\n",
      "classifier.3.weight\n",
      "classifier.3.bias\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "ckpt = torch.load(\"/media/yuganlab/blackstone/clip_classifier_epoch2.pt\", map_location=\"cpu\")\n",
    "print(\"\\n\".join(k for k in ckpt.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3204f897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/yuganlab/blackstone/xinlong/.conda/envs/ss2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_1229741/918833328.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"/media/yuganlab/blackstone/clip_classifier_epoch3.pt\", map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import KandinskyV22PriorPipeline, AutoPipelineForImage2Image, AutoPipelineForInpainting\n",
    "from transformers import CLIPTextModelWithProjection, CLIPTokenizer, CLIPVisionModelWithProjection\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "\n",
    "ckpt = torch.load(\"/media/yuganlab/blackstone/clip_classifier_epoch3.pt\", map_location=\"cpu\")\n",
    "\n",
    "# 2) Pull out the 'backbone' sub‑dict and rename to 'text_model'\n",
    "# vis_sd = {}\n",
    "# for k,v in ckpt.items():\n",
    "#     # only take the visual‐backbone keys if your backbone was actually the text encoder;\n",
    "#     # adjust the prefix if you used a different attribute name\n",
    "#     if k.startswith(\"backbone.\"):\n",
    "#         new_k = k.replace(\"backbone.\", \"vision_model.\")\n",
    "#         vis_sd[new_k] = v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "356299f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 6/6 [00:01<00:00,  4.30it/s]\n",
      "Keyword arguments {'prior': PriorTransformer(\n",
      "  (time_proj): Timesteps()\n",
      "  (time_embedding): TimestepEmbedding(\n",
      "    (linear_1): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (act): SiLU()\n",
      "    (linear_2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  )\n",
      "  (proj_in): Linear(in_features=1280, out_features=2048, bias=True)\n",
      "  (embedding_proj): Linear(in_features=1280, out_features=2048, bias=True)\n",
      "  (encoder_hidden_states_proj): Linear(in_features=1280, out_features=2048, bias=True)\n",
      "  (transformer_blocks): ModuleList(\n",
      "    (0-19): 20 x BasicTransformerBlock(\n",
      "      (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn1): Attention(\n",
      "        (to_q): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        (to_k): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        (to_v): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        (to_out): ModuleList(\n",
      "          (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (1): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm3): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff): FeedForward(\n",
      "        (net): ModuleList(\n",
      "          (0): GELU(\n",
      "            (proj): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          )\n",
      "          (1): Dropout(p=0.0, inplace=False)\n",
      "          (2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm_out): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  (proj_to_clip_embeddings): Linear(in_features=2048, out_features=1280, bias=True)\n",
      "), 'tokenizer': CLIPTokenizer(name_or_path='/home/yuganlab/.cache/huggingface/hub/models--kandinsky-community--kandinsky-2-2-prior/snapshots/9fc51ad5732afc5d031724219d22e6c42179c5a8/tokenizer', vocab_size=49408, model_max_length=77, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t49406: AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t49407: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n",
      "), 'text_encoder': CLIPTextModelWithProjection(\n",
      "  (text_model): CLIPTextTransformer(\n",
      "    (embeddings): CLIPTextEmbeddings(\n",
      "      (token_embedding): Embedding(49408, 1280)\n",
      "      (position_embedding): Embedding(77, 1280)\n",
      "    )\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-31): 32 x CLIPEncoderLayer(\n",
      "          (self_attn): CLIPSdpaAttention(\n",
      "            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): GELUActivation()\n",
      "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (text_projection): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "), 'clip_image_encoder': CLIPVisionModelWithProjection(\n",
      "  (vision_model): CLIPVisionTransformer(\n",
      "    (embeddings): CLIPVisionEmbeddings(\n",
      "      (patch_embedding): Conv2d(3, 1664, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "      (position_embedding): Embedding(257, 1664)\n",
      "    )\n",
      "    (pre_layrnorm): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-47): 48 x CLIPEncoderLayer(\n",
      "          (self_attn): CLIPSdpaAttention(\n",
      "            (k_proj): Linear(in_features=1664, out_features=1664, bias=True)\n",
      "            (v_proj): Linear(in_features=1664, out_features=1664, bias=True)\n",
      "            (q_proj): Linear(in_features=1664, out_features=1664, bias=True)\n",
      "            (out_proj): Linear(in_features=1664, out_features=1664, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): GELUActivation()\n",
      "            (fc1): Linear(in_features=1664, out_features=8192, bias=True)\n",
      "            (fc2): Linear(in_features=8192, out_features=1664, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_layernorm): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (visual_projection): Linear(in_features=1664, out_features=1280, bias=False)\n",
      "), 'clip_image_processor': CLIPImageProcessor {\n",
      "  \"crop_size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 224\n",
      "  }\n",
      "}\n",
      "} are not expected by KandinskyV22InpaintCombinedPipeline and will be ignored.\n",
      "Loading pipeline components...: 100%|██████████| 3/3 [00:00<00:00,  6.66it/s]\n",
      "Keyword arguments {'clip_image_encoder': CLIPVisionModelWithProjection(\n",
      "  (vision_model): CLIPVisionTransformer(\n",
      "    (embeddings): CLIPVisionEmbeddings(\n",
      "      (patch_embedding): Conv2d(3, 1664, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "      (position_embedding): Embedding(257, 1664)\n",
      "    )\n",
      "    (pre_layrnorm): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-47): 48 x CLIPEncoderLayer(\n",
      "          (self_attn): CLIPSdpaAttention(\n",
      "            (k_proj): Linear(in_features=1664, out_features=1664, bias=True)\n",
      "            (v_proj): Linear(in_features=1664, out_features=1664, bias=True)\n",
      "            (q_proj): Linear(in_features=1664, out_features=1664, bias=True)\n",
      "            (out_proj): Linear(in_features=1664, out_features=1664, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): GELUActivation()\n",
      "            (fc1): Linear(in_features=1664, out_features=8192, bias=True)\n",
      "            (fc2): Linear(in_features=8192, out_features=1664, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_layernorm): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (visual_projection): Linear(in_features=1664, out_features=1280, bias=False)\n",
      "), 'clip_image_processor': CLIPImageProcessor {\n",
      "  \"crop_size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 224\n",
      "  }\n",
      "}\n",
      "} are not expected by KandinskyV22PriorPipeline and will be ignored.\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:01<00:00,  5.60it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = torch.device(\"cuda:1\")\n",
    "\n",
    "pipe_prior = KandinskyV22PriorPipeline.from_pretrained(\n",
    "    \"kandinsky-community/kandinsky-2-2-prior\", torch_dtype=torch.float16, use_safetensors=True\n",
    ").to(device)\n",
    "\n",
    "vision_cfg     = pipe_prior.image_encoder.config\n",
    "ft_vis_encoder = CLIPVisionModelWithProjection(vision_cfg).to(device)\n",
    "\n",
    "text_cfg     = pipe_prior.text_encoder.config\n",
    "ft_text_encoder = CLIPTextModelWithProjection(text_cfg).to(device)\n",
    "\n",
    "ft_vis_encoder.load_state_dict(ckpt, strict=False)\n",
    "ft_text_encoder.load_state_dict(ckpt, strict=False)\n",
    "\n",
    "\n",
    "# ── E) Swap it into the pipeline ───────────────────────────────────────────\n",
    "\n",
    "pipe_prior.image_encoder = ft_vis_encoder\n",
    "pipe_prior.text_encoder = ft_text_encoder\n",
    "\n",
    "pipe_prior.text_encoder    = pipe_prior.text_encoder.to(device).float()\n",
    "pipe_prior.image_encoder = pipe_prior.image_encoder.to(device).float()\n",
    "pipe_prior.prior           = pipe_prior.prior.to(device).float()\n",
    "\n",
    "\n",
    "pipe = AutoPipelineForInpainting.from_pretrained(\n",
    "    \"kandinsky-community/kandinsky-2-2-decoder-inpaint\",\n",
    "    prior=pipe_prior.prior,\n",
    "\n",
    "    # 2) text side\n",
    "    tokenizer=pipe_prior.tokenizer,\n",
    "    text_encoder=pipe_prior.text_encoder,\n",
    "\n",
    "    # 3) image‑prompt side\n",
    "    clip_image_encoder=pipe_prior.image_encoder,\n",
    "    clip_image_processor=pipe_prior.image_processor,\n",
    "\n",
    "    # misc\n",
    "    torch_dtype=torch.float32,\n",
    "    use_safetensors=True\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ecb0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 30.75it/s]\n",
      "100%|██████████| 150/150 [00:11<00:00, 12.97it/s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image_path = \"/media/yuganlab/blackstone/HZL/Retinal_seg/Retinal-vessel-segmentation/data/test/image/01_test_0.png\"\n",
    "original_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "mask_path = \"/media/yuganlab/blackstone/HZL/Retinal_seg/Retinal-vessel-segmentation/data/test/mask/01_test_0.png\"\n",
    "mask_image = Image.open(mask_path).convert(\"L\")\n",
    "prompt = \"a binary mask of an eyeball vascular strucutre with increased tortuosity, vascular strucutre only\"\n",
    "out = pipe(\n",
    "    prompt= \"increase vessel tortuosity to high tortuosity level, keep binary mask of vascular structure\",  # what you want\n",
    "    image=original_image,               # your starting image\n",
    "    mask_image=mask_image,                    # None → pure img2img       # how strongly to follow your prompt\n",
    "    num_inference_steps=150,\n",
    "    generator=torch.Generator(device=device).manual_seed(1234),\n",
    ").images[0]\n",
    "\n",
    "out.save(\"clip_gen.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ss2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
